---
title: "Final Project Report on Grocery Chain Sales Forecasting"
author: "Tao Song (ts3089), Yangxue Lei (yl3805), Yu Zhou (yz3256)"
date: "December 1st, 2017"
output:
  prettydoc::html_pretty:
    theme: architect
    highlight: github
always_allow_html: yes
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, eval = TRUE)
```

# Grocery Sales Prediction Documents
This is an R Markdown format used for publishing markdown documents report on **Grocery Chain Sales Forecasting**. When you click the **Knit** button all R code chunks are run and a markdown file (.md) suitable for publishing to GitHub is generated.

# Intrdoduction
Grocery stores always need to decide on the products purchasing to balance the cost of
overstock and the demand of popular products by forecasting the product sales.
Different from such simple optimization problems we met in university, this realistic
Grocery Chain Sales Forecasting Problem is much more interesting and complex
considering the large amount of products, the seasonal tastes of customers, the
unpredictable product marketing and the various grocery shops location. In this project
we plan to conduct data cleaning and analysis with the raw history data, then build the
forecasting model to ensure the customers can get right amount of products at the right
time and the grocery stores can maximum their profits utilizing the machine learning
techniques.

# Data Source
*https://www.kaggle.com/c/favorita-grocery-sales-forecasting/data

# Data Description
A series of datasets will be used in this project which include various information about the grocery chain operation. The main datasets for analysis are train.csv and test.csv. The rest of files include supplementary information for modelling. The details are listed below:  
● train.csv & test.csv: provide the target unit_sales by date, store_nbr, item_nbr and a
unique id to label rows for training and testing respectively.  
● stores.csv: store metadata, including city, state, type, and cluster (a grouping of similar
stores).  
● items.csv: Item metadata, including family, class, and perishable.  
● transactions.csv: the count of sales transactions for each date, store_nbr combination.  
● holidays_events.csv: holidays and events information

# Data Preparation and Exploration
## 1, Data Preparation
1.0, Load Packages and Read Dataset
```{r load packages and function, include=FALSE}
library(data.table)
library(DT)
library(dplyr)
library(ggplot2)
library(plyr)
library(lubridate)
```

```{r read dataset, echo=TRUE}
raw_train_dt <- fread(input = '../GummyLions/In_Use/Data/train.csv')
holidays_events_dt <- fread(input='../GummyLions/In_Use/Data/holidays_events.csv')
items_dt <- fread(input = '../GummyLions/In_Use/Data/items.csv')
oil_dt <- fread(input = '../GummyLions/In_Use/Data/oil.csv')
stores_dt <- fread(input = '../GummyLions/In_Use/Data/stores.csv')
transactions_dt <- fread(input = '../GummyLions/In_Use/Data/transactions.csv')
```
1.1, Sample and aggregate dataset
```{r sample data, eval=FALSE, echo =FALSE}
sample.list <- items_dt[family %in% c("MEATS"),item_nbr]
train_dt <- raw_train_dt[item_nbr %in% sample.list,]
train_dt[,`:=`(date, as.Date(train_dt$date))]
```
Due to the high volume of data, we sample only a typical category family--meats, as our analysis and modeling focus.
1.2, Help function
```{r help function}
plot.categoric <- function(dt,col,color = 2){
  num.plot <- ggplot(dt, aes(x=get(col))) + 
    geom_bar(fill = sample[color], alpha = 0.7) + 
    geom_text(aes(label = ..count..), stat='count', vjust=-0.5) + 
    theme_minimal() + 
    xlab(col) +
    ylab("Freq") +
    theme(axis.text.x = element_text(angle = 15, size=10))
  return(num.plot)
}

plot.histogram <- function(dt,col,color = 5){
  num.plot <- ggplot(dt, aes(x=get(col))) + 
    geom_histogram(breaks=seq(0,70,by=10), fill = sample[color], alpha = 0.8) + 
    theme_minimal() + 
    xlab(col)
    theme(axis.text.x = element_text(angle = 15, size=10))
  return(num.plot)
}

plot.scatter <- function(dt,x,y,color = 3){
  num.plot <- ggplot(dt, aes(x=get(x),y=get(y))) + 
    geom_point(size = 1, color = sample[color] ) +
    geom_line() +
    xlab(x)+
    ylab(y)
  return(num.plot)
}
#define color set
color <- c("#E69F00", "#56B4E9", "#009E73", "#F0E442", "#0072B2", "#D55E00", "#FF3333")
```

## 2, Data Summary

2.1.1, Train and Test Variable Identification

Training data, which includes the target *unit_sales* by date, *store_nbr*, and *item_nbr* and a unique *id* to label rows:

**id**: Unique identification for every record.
**date**: Date information for every daily record.
**store_nbr**: Unique identification for *store number*.
**item_nbr**: Unique identification for *item number*. 
**unit_sales**: The *unit_sales* for every item, could be integer (a bag of chips) or float (1.5 kg of cheese). Negative values of *unit_sales* represent returns of that particular item.
**onpromotion**: Binary variable *onpromotion* indicates whether that item_nbr was on promotion for a specified date and store_nbr. Approximately 16% of the *onpromotion* values in this file are *NaN*.

**NOTE**: The training data does not include rows for items that had zero unit_sales for a store/date combination. There is no information as to whether or not the item was in stock for the store on the date, and teams will need to decide the best way to handle that situation. Also, there are a small number of items seen in the training data that aren't seen in the test data.

```{r check train dataset summary}
summary(train_dt[,c("store_nbr", "item_nbr","unit_sales","onpromotion")])
length <- nrow(train_dt)
max.date <- max(train_dt$date)
min.date <- min(train_dt$date)
```
In total the train dataset contains `r length.id` lines of record. The date ranges from `r min.week` to `r max.week`. The table summarize the main variables in train dataset.
```{r check train dataset}
first.lines <- 20
datatable(train_dt[1:first.lines,])
```
The table shows the head of data and parts of sample dataset. 

2.1.2 Explore Sales and Item Sales in Train Dataset
```{r explore daily item sales}
daily.item.sales <- train_dt[,lapply(.SD, FUN="sum"), .SDcol=c("unit_sales"), by=c("date") ]
daily.sales.item.plot <- ggplot(daily.item.sales, aes(x=date,y=unit_sales, fill=unit_sales)) +
  geom_line() +
  xlab("Date") +
  ylab("Item Sales") +
  ggtitle("Daily Item Sales Trend") +
  theme_minimal()
daily.sales.item.plot
```
The initial line graph of daily item sales shows great fluctuation in new year. Considering in most cirsumstance the sales in the begin of new year would decrease draftly due to the close of store. Usually the item sales would fluctuate within 20000 to 30000, however achieve as high as 110000 some day. We consider that might be a fat figure error and tried to remove that data point.
```{r explore remove outlier}
train_dt <- train_dt[unit_sales != max(train_dt$unit_sales),]
daily.item.sales <- train_dt[date >= as.Date("2016-01-01"),lapply(.SD, FUN="sum"), .SDcol=c("unit_sales"), by=c("date") ]
daily.sales.item.plot <- ggplot(daily.item.sales, aes(x=date,y=unit_sales, fill=unit_sales)) +
  geom_line() +
  xlab("Date") +
  ylab("Item Sales") +
  ggtitle("Daily Item Sales Trend after 2016") +
  theme_minimal()
daily.sales.item.plot
```
After removing the outlier and focusing on a short window period, the trend and fluctuation would be much more obvious. Every week the total amount of item sales might be similar and in distict weekday the item sales vary greatly.

2.1.3 Explore Item Sales by Day of Week
```{r explore item sales by week day}
week.day.sales <- {{train_dt[date >= as.Date("2016-01-01"),lapply(.SD, FUN="sum"), .SDcol=c("unit_sales"), by=c("date") ] %>% data.table()} [,`:=`(weekday, weekdays(date))] %>% data.table()} [,lapply(.SD, FUN="sum"), .SDcols=c("unit_sales"), by=c("weekday")]

week.day.sales.plot <- ggplot(week.day.sales, aes(x=reorder(weekday,-unit_sales), y=unit_sales, fill=reorder(weekday,-unit_sales))) +
  geom_bar(width = 0.65, stat="identity") +
  coord_polar("y", start=0) +
  xlab("Unit Sales") +
  ylab("By Week Day") +
  ggtitle("Unit Sales by Week Day") +
  guides(fill=guide_legend(title="Day of Week")) +
  theme_minimal()
week.day.sales.plot
```
2.1.4 Explore Monthly Item Sales
```{r explore monthly item sales}
monthly.item.sales <- {{train_dt[date<"2017-08-01",] %>% data.table()}[,`:=`(by.month, format(as.Date(date),"%Y-%m"))] %>% data.table()} [,lapply(.SD, FUN="sum"), .SDcol=c("unit_sales"), by=c("by.month") ]
monthly.item.sales[,`:=`(by.month, paste(by.month,"01",sep="-"))]
monthly.item.sales.plot <- ggplot(monthly.item.sales, aes(x=as.Date(by.month,"%Y-%m-%d"), y=unit_sales, fill=unit_sales)) +
  geom_line() +
  xlab("Date") +
  ylab("Monthly Item Sales") +
  ggtitle("Monthly Item Sales Trend") +
  theme_minimal()
monthly.item.sales.plot
```
2.1.5 Explore Daily Sales
```{r explore daily sales}
daily.sales <- train_dt[,.N, by=c("date") ]
daily.sales.plot <- ggplot(daily.sales, aes(x=as.Date(date),y=N, fill=N)) + 
  geom_line(color = "blue") +
  xlab("Date") +
  ylab("Daily Sales") +
  ggtitle("Daily Sales Trend") +
  theme_minimal()
daily.sales.plot
```
2.1.6 Explore Monthly Sales
```{r explore monthly sales}
monthly.sales <- {{train_dt[date<"2017-08-01",] %>% data.table()} [,`:=`(by.month, format(as.Date(date),"%Y-%m"))] %>% data.table()} [,.N, by=c("by.month") ]
monthly.sales[,`:=`(by.month, paste(by.month,"01",sep="-"))]
monthly.sales.plot <- ggplot(monthly.sales, aes(x=as.Date(by.month,"%Y-%m-%d"), y=N)) +
  geom_line(color = "blue") +
  xlab("Date") +
  ylab("Monthly Sales") +
  ggtitle("Monthly Sales Trend") +
  theme_minimal()
monthly.sales.plot
```
From the item sales by day and month, the item sales fluctuate and have obvious seational effect. The item sales would soar during the end of the year, which is exactly *Christmas Holidays*. After the holiday, both of item sales and sales would have a significant decreasement at the begin of new year. 

2.1.7 Explore Sales by Distinct Store
```{r explore store_nbr sales}
factor_train_dt <- {train_dt[unit_sales<30 & unit_sales>-10,] %>% data.table()} [,`:=`(store_nbr, as.factor(store_nbr))]
store_nbr.plot <- ggplot(factor_train_dt, aes(x=store_nbr, y=unit_sales, fill=store_nbr))+
  geom_boxplot(outlier.shape=NA) +
  guides(fill=FALSE) +
  xlab("Store_nbr") +
  ylab("Sales by store_nbr") +
  ggtitle("Sales by Store Boxplot") +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 90, size=8))
store_nbr.plot
```
From the boxplot the sales number in distinct stores have great difference, the average sales number with highest value might be tens of lowest one.

2.2.1, Summary Item Information
```{r explore item number}
top.item.sales <- {train_dt[, .N, by=c("item_nbr")] %>% data.table()} [order(-N)] %>% setnames(old="N",new="Count")
datatable(top.item.sales[1:20,])
```
2.2.2, Explore Daily Item Sales Distribution
```{r explore daily item sales distribution}
library(e1071)
daily.sales.distribution <- train_dt[, lapply(.SD, FUN="sum"), .SDcols=c("unit_sales"), by=c("date")]
daily.sales.mean <- mean(daily.sales.distribution$unit_sales)
daily.sales.variance <- sd(daily.sales.distribution$unit_sales)
daily.sales.skewness <- skewness(daily.sales.distribution$unit_sales)
daily.sales.kurtosis <- kurtosis(daily.sales.distribution$unit_sales)
daily.sales.distribution.plot <- ggplot(daily.sales.distribution, aes(unit_sales)) + 
    geom_histogram(aes(y=..density..),breaks = seq(10000,40000,by=1000),fill = color[2], alpha = 0.8) +
    geom_density(color = color[3], alpha = 0.5) +
    xlab("Unit Sales") +
    ylab("Density") +
    ggtitle("Daily Sales Histogram")
    theme_minimal()
daily.sales.distribution.plot
```
2.2.3, Explore Log Daily Item Sales Distribution
```{r explore log daily item sales distribution}
daily.sales.distribution[,`:=`(log_unit_sales, log(unit_sales))]

log.daily.sales.mean <- mean(daily.sales.distribution$log_unit_sales)
log.daily.sales.variance <- sd(daily.sales.distribution$log_unit_sales)
log.daily.sales.skewness <- skewness(daily.sales.distribution$log_unit_sales)
log.daily.sales.kurtosis <- kurtosis(daily.sales.distribution$log_unit_sales)
log.daily.sales.distribution.plot <- ggplot(daily.sales.distribution, aes(log_unit_sales)) + 
    geom_histogram(aes(y=..density..), breaks=seq(9, 11, by=0.1), fill = "cyan", alpha = 0.5) + 
    geom_density(color = color[3], alpha = 0.5) +
    xlab("Unit Sales") +
    ylab("Density") +
    ggtitle("Daily Log-Sales Histogram")
    theme_minimal()
log.daily.sales.distribution.plot
```
From the daily item sales histogram, the daily item sales distribution has a heavy tail and right skewed distribution, with mean value `r daily.sales.mean` and standard deviation `r daily.sales.variance`. The skewness is `r daily.sales.kurtosis` and kurtosis is `r daily.sales.kurtosis`. After log transformation, the log daily item sales distrbution would approach more to normal distribution, wuth mean value `r log.daily.sales.mean` and standard deviation `r log.daily.sales.variance`. The skewness is `r log.daily.sales.kurtosis` and kurtosis is `r log.daily.sales.kurtosis`

2.2.4, Explore Promotion
```{r explore promotion}
onpromotion.count <- train_dt[, .N, by=c("onpromotion")] %>% setnames(old="N", new="number")
 sale.plot <- ggplot(onpromotion.count, aes(x="", y=number, fill=onpromotion)) + 
  geom_bar(width = 0.5, stat="identity") +
  coord_polar("y", start=0) +
  theme_minimal()
sale.plot
```
From the bar table above, only a few number of items are sold on promotion, while almost 65% of them are sold without promotion. Also about 25% of the data miss.

2.2.5 Explore Holiday
```{r check holidays}
datatable(holidays_events_dt)
```
**date**: Holiday date information.
**type**: *Holiday* indicate normal celebration days. *Additional* holidays are days added a regular calendar holiday, for example, as typically happens around Christmas (making Christmas Eve a holiday). Days that are type *Bridge* are extra days that are added to a holiday (e.g., to extend the break across a long weekend). These are frequently made up by the type Work Day which is a day not normally scheduled for work (e.g., Saturday) that is meant to payback the Bridge.
**locale**: Indication of holiday locality attributes, including national, regional and local attribute.
**transferred**: Whether the holiday is transfered. Pay special attention to the transferred column. A holiday that is transferred officially falls on that calendar day, but was moved to another date by the government. A transferred day is more like a normal day than a holiday. To find the day that it was actually celebrated, look for the corresponding row where type is Transfer. For example, the holiday Independencia de Guayaquil was transferred from 2012-10-09 to 2012-10-12, which means it was celebrated on 2012-10-12.
```{r explore holiday type}
holiday.type.plot <- ggplot(holidays_events_dt, aes(x=type)) + 
    geom_bar(fill = color[3], alpha = 0.6, width = 0.5) + 
    geom_text(aes(label = ..count..), stat='count', vjust=-0.4) + 
    theme_minimal() + 
    xlab("Type") +
    ylab("Freq") +
    ggtitle("Holiday Histogram")
    theme(axis.text.x = element_text(angle = 15, size=10))
holiday.type.plot
```
From the histogram most of the holidays are real holidays: most of holidays, additional days, bridge days and event, while a small part of holidays are tranfered.
```{r explore locale}
holiday.locale <- holidays_events_dt[, .N, by=c("locale")] %>% setnames(old="N", new="number")
holiday.locale.plot <- ggplot(holiday.locale, aes(x="", y=number, fill=locale)) + 
  geom_bar(width = 0.5, stat="identity") +
  coord_polar("y", start=0) +
  scale_fill_brewer(palette="Blues") +
  theme_minimal()
holiday.locale.plot 
```
From the bar chart about 50% of the holidays belong to national festivals and 45% of the holidays are local holidays. Only a small piece of holidays, roughly 10% would be regional holidays.
```{r explore holiday tranfer}
holiday.transferred <- holidays_events_dt[, .N, by=c("transferred")] %>% setnames(old="N", new="number")
holiday.transferred.plot <- ggplot(holiday.transferred, aes(x="", y=number, fill=transferred)) + 
  geom_bar(width = 0.5, stat="identity") +
  coord_polar("y", start=0) +
  scale_fill_brewer(palette="Greens") +
  theme_minimal()
holiday.transferred.plot
```
From the bar chart most of the holidays are normal and only 10% of them are transferred to another days.

2.3.1 Explore Item Attribution
```{r check items}
datatable(items_dt[1:first.lines,])
```
Item metadata, including family, class, and perishable.
**item_nbr**: Unique identification for item number.
**family**: The category of item.
**class**: The class of item.
**perishable**: Whether the item is perishable. And items marked as perishable have a score weight of 1.25; otherwise, the weight is 1.0.
```{r explore item family}
largest.item.family.plot <- ggplot(items_dt, aes(x=family)) + 
    geom_bar(fill = color[1], alpha = 0.8, width = 0.8) + 
    geom_text(aes(label = ..count..), stat='count', vjust=-0.1) + 
    theme_minimal() + 
    xlab("Family") +
    ylab("Item count") +
    theme(axis.text.x = element_text(angle = 90, size=5))
largest.item.family.plot
```

```{r explore item parishable}
item.perishable <- items_dt[, .N, by=c("perishable")] %>% setnames(old="N", new="number")
item.number <- nrow(items_dt)
item.perishable[,`:=`(perishable, mapvalues(item.perishable$perishable,from = c(0,1), to=c("False","True")))]
item.perishable.plot <- ggplot(item.perishable, aes(x="", y=number, fill=perishable)) + 
  geom_bar(width = 0.5, stat="identity") +
  coord_polar("y", start=0) +
  scale_fill_brewer(palette="Oranges") +
  theme_minimal()
item.perishable.plot
```
From the bar chart among the `r item.number` pieces of items, about 25% of them are perishable, and this types of items should be allocated higher weight than unperishable items. The competition pages states that perishable items are weigthed more heavily than non-perishables. Shorter shelf lives mean a smaller margin of error when it comes to forecasting sales.

2.4.1 Explore Oil Price
```{r check oil price}

price.plot <- ggplot(oil_dt, aes(x=as.Date(date),y=dcoilwtico, fill=dcoilwtico)) + 
  geom_line(color=color[2]) +
  geom_area(fill="cyan", alpha=0.08) +
  xlab("Date") +
  ylab("Oil price") +
  ggtitle("Oil Price Fluctuation") +
  theme_minimal()
price.plot
```
We see that oil prices suffered a collapse towards the end of 2014 and have not recovered. In fact despite some volatility, oil prices are at the same level as they were in the beginning of 2015. As a result of this we may see a significant shift in store sales around late 2014. Looking at the unit sales data, this is not readily apparent. Although sales do appear to drop off in the early part of 2015, in late 2014 they are rising.

2.5.1 Explore Store, City and State
```{r check stores}
datatable(stores_dt)
```
```{r explore sales by city}
new_train_dt <- merge(train_dt, stores_dt, by.x="store_nbr", by.y="store_nbr", all.x=TRUE)
sales.by.city <- new_train_dt[, lapply(.SD, FUN="sum"), .SDcols=c("unit_sales"), by=c("city")]

sales.by.city.plot <- ggplot(sales.by.city, aes(x=city, y=unit_sales)) + 
  geom_bar(stat="identity", fill = color[4], alpha = 0.8, width = 0.8) +
  #geom_bar(fill = color[4], alpha = 0.8, width = 0.8) +
  #geom_text(aes(label = ..identity..), stat='count', vjust=-0.3) + 
    theme_minimal() + 
    xlab("City") +
    ylab("Unite Sales") +
    theme(axis.text.x = element_text(angle = 50, size=5))
sales.by.city.plot
```
From the graph Quito city, the capital and population center of the contry, takes the largest share of marke and is far higher than any other city.
```{r explore sales by state}
new_train_dt <- merge(train_dt, stores_dt, by.x="store_nbr", by.y="store_nbr", all.x=TRUE)
sales.by.state <- new_train_dt[, lapply(.SD, FUN="sum"), .SDcols=c("unit_sales"), by=c("state")]

sales.by.state.plot <- ggplot(sales.by.state, aes(x=state, y=unit_sales)) + 
  geom_bar(stat="identity", fill = color[6], alpha = 0.7, width = 0.8) +
    theme_minimal() + 
    xlab("State") +
    ylab("Unite Sales") +
    theme(axis.text.x = element_text(angle = 50, size=5))
sales.by.state.plot
```
From the graph Pichicha province, where the capital of Quito city locates and population aggregates, takes the largest share of marke and is far higher than any other state.
```{r check transaction data}
datatable(transactions_dt[1:first.lines,])
```
2.6.1 Explore Transaction
```{r explore transactions}
trans_dt <- transactions_dt[,lapply(.SD, FUN="sum"), .SDcols=c("transactions"), by=c("store_nbr")]
transaction.plot <- ggplot(trans_dt, aes(x=store_nbr,y=transactions)) +
  geom_bar(stat="identity",fill= color[7], alpha=0.6) +
  scale_y_continuous(labels = scales::unit_format()) +
  guides(fill=FALSE) +
  xlab("Transaction Number") +
  ylab("Transaction by store") +
  ggtitle("Transactions by Store Histgram") +
  theme_minimal()
transaction.plot
```
For this competition we are also provided with transactions, which is a count of transactions per store per day. One transaction would be one customer buying 1 or more products - basically one customer going through the cashier. Let’s first see what these look like over time. After plotting the transaction number by store, still we could conclude that in some stores the average transaction would be much higher than others.
```{r transaction sales ratio}
temp <- train_dt[, sum(unit_sales), by=c("date","store_nbr") ] %>% setnames(old="V1",new="Item_Sales")
transactions_dt <- transactions_dt[,`:=`(date, as.Date(date))]
trans_sales <- merge(temp, transactions_dt, by.x=c("date","store_nbr"), by.y=c("date","store_nbr"))
trans_sales <- {trans_sales[,`:=`(ratio, Item_Sales/transactions)] %>% data.table()} [,lapply(.SD, FUN="sum"), .SDcols=c("Item_Sales","transactions","ratio"), by=c("date")]

trans_sales <- trans_sales[date <= as.Date("2016-01-01") | date >= as.Date("2016-01-05"),]
item.sales.ratio.plot <- ggplot(trans_sales, aes(x=as.Date(date), y=ratio, fill=ratio)) +
  geom_line() +
  xlab("Date") +
  ylab("Item Sales Ratio") +
  ggtitle("Daily Item Sales Ratio Trend") +
  theme_minimal()
item.sales.ratio.plot
```
After checking the dataset, from 2016-01-02 to 2016-01-04 the *item sales ratio* was sharply higher than usual, after removing these periods the trend and fluctuation had become much easier to identify.

#Modeling with sample dataset
3.1 
```{r load library}
library(plyr)
library(forecast)
library(tseries)
#read meat data and split to train and test
meats_dt <- fread(input = '../GummyLions/In_Use/Data/meats_dt.csv')
meats_train <- meats_dt[store_nbr %in% c(3,44,45,47) & date < "2017-08-01", ]
meats_test <- meats_dt[store_nbr %in% c(3,44,45,47) & date >= "2017-08-01", ]
meats_train[, ':='(unit_sales,log(1+unit_sales))]
meats_train[, ':='(unit_sales, mapvalues(meats_train$unit_sales, c(NaN), c(0)))]
meats_train[, ':='(date, as.Date(date))]

#days of the week factor
meats_train[, ':='(dow, weekdays(date))]
weekly_factor <- meats_train[, lapply(.SD, mean), .SDcols = c("unit_sales"), by = c('store_nbr', 'item_nbr', 'dow')]
setnames(weekly_factor, "unit_sales", "average_dow")
weekly_factor[, ':='(average, mean(average_dow)), by = c('store_nbr', 'item_nbr')]
weekly_factor[, ':='(factor, average_dow/average)]
weekly_factor[, ':='(factor, mapvalues(weekly_factor$factor, c(NaN), c(0)))]

#moving averages
last_date <- as.Date('2017-07-31')
ma <- meats_train[date == last_date, lapply(.SD, mean), .SDcols = "unit_sales", by = c('store_nbr', 'item_nbr')]
setnames(ma, "unit_sales", "ma0")
for (i in 1:16){
  date_cst <- last_date - as.difftime(i*7, unit = "days")
  temp <- meats_train[date >= date_cst, lapply(.SD, mean, na.rm=TRUE), .SDcols = "unit_sales", by = c('store_nbr', 'item_nbr')]
  setnames(temp, "unit_sales", paste("ma", toString(i), sep =''))
  ma <- merge(ma, temp, by = c("store_nbr", "item_nbr"), all = TRUE)
}
ma[, ':='(ma_median, apply(ma[,3:18], 1, median, na.rm=TRUE))]


#calculate prediction for test data
meats_test <- merge(meats_test, ma[, c('store_nbr','item_nbr','ma_median')], by = c('store_nbr', 'item_nbr'), all.x = TRUE)
meats_test[, ':='(dow, weekdays(as.Date(date)))]
meats_test <- merge(meats_test, weekly_factor[, c('store_nbr','item_nbr', 'dow','factor')], by = c('store_nbr', 'item_nbr', 'dow'), all.x = TRUE)
meats_test[, ':='(pred, exp(ma_median*factor)-1)]
meats_test[, ':='(NWSLE, perishable * (log(pred+1)-log(unit_sales+1))**2)]
meats_test_NWRMSLE <- meats_test[, lapply(.SD, sum), .SDcols = c("perishable", "NWSLE"), by = c("store_nbr", "item_nbr")]
setnames(meats_test_NWRMSLE, c("perishable", "NWSLE"), c("w_sum","NWSLE_sum"))
meats_test_NWRMSLE[, ':='(NWRMSLE, sqrt(NWSLE_sum / w_sum))]
summary(meats_test_NWRMSLE$NWRMSLE)

#adding the simple effect of promotion, when promotion, sales_unit * p_factor
p_factor <- 0.8
meats_test[,':='(onpromotion, mapvalues(meats_test$onpromotion, c(NA), c(FALSE)))]
meats_test[onpromotion == TRUE, ':='(pred_w_promo, pred*p_factor)]
meats_test[onpromotion == FALSE, ':='(pred_w_promo, pred)]
meats_test[,':='(NWSLE_wP, perishable * (log(pred_w_promo+1)-log(unit_sales+1))**2)]
meats_test_NWRMSLE_wP <- meats_test[, lapply(.SD, sum), .SDcols = c("perishable", "NWSLE_wP"), by = c("store_nbr", "item_nbr")]
setnames(meats_test_NWRMSLE_wP, c("perishable", "NWSLE_wP"), c("w_sum_wP","NWSLE_sum_wP"))
meats_test_NWRMSLE_wP[, ':='(NWRMSLE_wP, sqrt(NWSLE_sum_wP / w_sum_wP))]
summary(meats_test_NWRMSLE_wP$NWRMSLE_wP)
```


3.2.1
```{r prophet modeling prepare}
library(prophet)
train_dt <- fread('../GummyLions/In_Use/Data/meats_dt.csv')
store.list <- c(3,44,45,47)
meats.list.total <- unique(items_dt[family == "MEATS",]$item_nbr)
result <- {train_dt[item_nbr %in% meats.list.total & store_nbr %in% store.list,] %>% data.table()} [,.N,by=c("item_nbr","store_nbr")]
n <- {result[,.N,by=c("item_nbr")] %>% data.table()} [N == 4,]
meats.list <- n[c(1,5,12,17,25,9),]$item_nbr
```
Read data from filtering data and limit dataset within parts of the records.

3.2.2
```{r prophet modeling}
table_a <- data.table()
table_b <- data.table()
for (i in store.list){
  for (j in meats.list){
    ts_dt <- train_dt[store_nbr == i & item_nbr == j,]
    ts_dt <- ts_dt[,`:=`(date, as.Date(date))] %>% setnames("unit_sales","y") %>% setnames("date","ds") 
    train <- ts_dt[ds < as.Date("2017-08-01"),c("ds","y")]
    test <- ts_dt[ds >= as.Date("2017-08-01"),c("ds","y")]

    m <- prophet(train,daily.seasonality=TRUE)
    future <- make_future_dataframe(m, periods = 15)
    head(future)
    forecast <- predict(m, future)
    forecast <- as.data.table(forecast)
    ft <- forecast[,c("ds","yhat")]
    ft <- ft[,`:=`(ds,as.Date(ds))]
    temp <- merge(ft,test,by.x="ds",by.y="ds")
    temp[,`:=`(diff, abs(y-round(yhat)))]
    temp <- temp[ds >= "2017-08-01",]
    temp[, ':='(NWSLE, (log(yhat+1)-log(y+1))**2)]
    NWSLE <- sqrt(sum(temp$NWSLE,na.rm=TRUE)/nrow(temp))
    a <- data.table(i,j,NWSLE)
    table_a <- rbind(table_a, a)
    b <- cbind(rep.int(i,times=nrow(forecast)),rep.int(j,times=nrow(forecast)),ft[,as.character(ds)],ft[,yhat])
    table_b <- rbind(table_b,b)
  }
}
prophet_NWRMSLE <- setnames(table_a,c("i","j","NWSLE"),c("store_nbr","item_nbr","NWRMSLE"))
prophet_prediction <- setnames(table_b,c("V1","V2","V3","V4"),c("store_nbr","item_nbr","date","pred"))
#mark item 1，5，12，17，25，9
```
Modeling with Facebook open source predictive tools *prophet* and calculate the NWRMSLE and prediction. 
















```{r modeling, eval = FLASE, echo= FALSE}
groceryII_dt <- fread(input = '../GummyLions/sample_groceryII.csv')
groceryII_dt[item_nbr %in% items_dt[perishable == 0,], ':='(perishable, 1)]

groceryII_dt[, ':='(perishable, mapvalues(groceryII_dt$perishable, c(NA), c(1.25)))]
#adding store information
groceryII_dt <- merge(groceryII_dt, stores_dt, by = c("store_nbr"))
#creating full trainable data
groceryII_dt <- merge(groceryII_dt, groceryII_dat[,!c("V1","id")], by = c("date" , "store_nbr", "item_nbr"), all.x = TRUE)
setorderv(groceryII_dt, c("date", "store_nbr", "item_nbr"))

groceryII_dt[unit_sales < 0, ':='(unit_sales, 0)]
groceryII_dt[, ':='(unit_sales, mapvalues(groceryII_dt$unit_sales, NA, 0))]
groceryII_dt[, ':='(dow, weekdays(as.Date(date)))]
```

```{r modeling with FDA, eval = FLASE, echo= FALSE}
grocery.list <- items_dt[family %in% c("GROCERY II"),]$item_nbr
sample_dt <- raw_train_dt[item_nbr %in% grocery.list,]
sample_dt <- merge(sample_dt, items_dt, by.x=c("item_nbr"), by.y=c("item_nbr"))
sample_dt[,`:=`(date, as.Date(date))]
sample_dt[,`:=`(Day_of_Week, weekdays(date))]
sample_dt[,`:=`(Day_of_Month, day(date))]
sample_dt[,`:=`(promotion_weight, mapvalues(onpromotion,c(NA,FALSE,TRUE),c(0,0,1)))]
sample_dt <- merge(sample_dt, stores_dt, by.x=c("store_nbr"), by.y=c("store_nbr"))
sample_dt[unit_sales < 0, ':='(unit_sales, 0)]
```

```{r add holiday effect, eval = FLASE, echo= FALSE}
holidays_events_local <- holidays_events_dt[locale == "Local" & transferred == FALSE, ]
sample_dt[(date %in% holidays_events_local$date) & (city %in% holidays_events_local$locale_name), ':='(transferred, FALSE)]
#adding regional holidays
holidays_events_regional <- holidays_events_dt[locale == "Regional" & transferred == FALSE, ]
sample_dt[date %in% holidays_events_regional$date & state %in% holidays_events_regional$locale_name, ':='(transferred, FALSE)]
#adding national holidays
holidays_events_national <- holidays_events_dt[locale == "National" & transferred == FALSE, ]
sample_dt[date %in% holidays_events_national$date, ':='(transferred, FALSE)]
```


```{r smooth spline, eval = FLASE, echo= FALSE}
sample_dt <- train_dt[store_nbr == 25 & item_nbr == 315463,]
x <- sample_dt$week
y <- sample_dt$unit_sales
s.hat <- smooth.spline(x, y)
```